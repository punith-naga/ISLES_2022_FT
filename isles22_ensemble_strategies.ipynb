{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ea3ff9",
   "metadata": {},
   "source": [
    "# ISLES'22 Ensemble Strategies with MONAI\n",
    "\n",
    "This notebook implements bagging, boosting, and stacking ensembles for ischemic stroke lesion segmentation on the ISLES'22 pre-processed dataset using MONAI. It assumes that the pre-processed data has been downloaded locally (see the setup cell below for expected directory structure).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034bb30c",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "- Python 3.10+\n",
    "- `torch` 2.0+ with CUDA if available\n",
    "- `monai` 1.3+\n",
    "- `numpy`, `pandas`, `matplotlib`, `scikit-learn`\n",
    "- Optional: `tqdm`, `tensorboard`\n",
    "\n",
    "Expected directory layout after downloading the ISLES'22 pre-processed data from Google Drive:\n",
    "\n",
    "```\n",
    "/data/isles22/\n",
    "    imagesTr/\n",
    "        case_0001.nii.gz\n",
    "        ...\n",
    "    labelsTr/\n",
    "        case_0001_seg.nii.gz\n",
    "        ...\n",
    "    imagesVal/\n",
    "    labelsVal/\n",
    "```\n",
    "\n",
    "Update `DATA_ROOT` in the configuration cell if your paths differ. If the dataset is not present, the notebook can be run in dry-run mode using a synthetic dataset for debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17089b8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:35.379485Z",
     "iopub.status.busy": "2025-11-02T09:51:35.379350Z",
     "iopub.status.idle": "2025-11-02T09:51:35.382658Z",
     "shell.execute_reply": "2025-11-02T09:51:35.381433Z"
    }
   },
   "outputs": [],
   "source": [
    "# If running in a fresh environment, uncomment the following line to install dependencies.\n",
    "# !pip install --upgrade pip\n",
    "# !pip install \"torch>=2.0\" \"monai-weekly[nibabel,tqdm,ignite]\" scikit-learn pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54738c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:35.385250Z",
     "iopub.status.busy": "2025-11-02T09:51:35.385111Z",
     "iopub.status.idle": "2025-11-02T09:51:38.481962Z",
     "shell.execute_reply": "2025-11-02T09:51:38.480878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.6.dev2544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 2.3.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.9.0+cu128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: f910be902091ce9efdc2928e1eb473fd607bfeb9\n",
      "MONAI __file__: /home/<username>/.local/lib/python3.12/site-packages/monai/__init__.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optional dependencies:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Ignite version: 0.4.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nibabel version: 5.3.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy version: 1.16.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillow version: 12.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard version: 2.20.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm version: 4.67.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psutil version: 7.1.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 2.3.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For details about installing the optional dependencies, please visit:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.transforms import (\n",
    "    Activationsd,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    RandAffined,\n",
    "    RandZoomd,\n",
    "    RandFlipd,\n",
    "    RandSpatialCropd,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print_config()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac49087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.484056Z",
     "iopub.status.busy": "2025-11-02T09:51:38.483822Z",
     "iopub.status.idle": "2025-11-02T09:51:38.488829Z",
     "shell.execute_reply": "2025-11-02T09:51:38.488385Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    data_root: Path = Path(\"/data/isles22\")\n",
    "    images_subdir: str = \"imagesTr\"\n",
    "    labels_subdir: str = \"labelsTr\"\n",
    "    val_images_subdir: Optional[str] = \"imagesVal\"\n",
    "    val_labels_subdir: Optional[str] = \"labelsVal\"\n",
    "    cache_rate: float = 0.1\n",
    "    num_workers: int = 4\n",
    "    roi_size: Tuple[int, int, int] = (96, 96, 64)\n",
    "    batch_size: int = 2\n",
    "    base_lr: float = 1e-4\n",
    "    max_epochs: int = 100\n",
    "    patience: int = 20\n",
    "    n_bagging_models: int = 5\n",
    "    boosting_rounds: int = 4\n",
    "    stacking_meta_epochs: int = 50\n",
    "    amp: bool = True\n",
    "    num_classes: int = 2\n",
    "    in_channels: int = 1\n",
    "    seed: int = 42\n",
    "    validation_split: float = 0.2\n",
    "    synthetic_samples: int = 4  # used for dry-run mode when data is unavailable\n",
    "\n",
    "\n",
    "cfg = ExperimentConfig()\n",
    "set_determinism(seed=cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12c22b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.491334Z",
     "iopub.status.busy": "2025-11-02T09:51:38.491180Z",
     "iopub.status.idle": "2025-11-02T09:51:38.494716Z",
     "shell.execute_reply": "2025-11-02T09:51:38.493587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found. Enable dry-run mode by generating synthetic data or update cfg.data_root.\n"
     ]
    }
   ],
   "source": [
    "def check_dataset_availability(cfg: ExperimentConfig) -> bool:\n",
    "    image_dir = cfg.data_root / cfg.images_subdir\n",
    "    label_dir = cfg.data_root / cfg.labels_subdir\n",
    "    available = image_dir.exists() and label_dir.exists()\n",
    "    if available:\n",
    "        print(f\"Found dataset at {cfg.data_root}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Dataset not found. Enable dry-run mode by generating synthetic data or update cfg.data_root.\"\n",
    "        )\n",
    "    return available\n",
    "\n",
    "\n",
    "data_available = check_dataset_availability(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada8d9f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.496242Z",
     "iopub.status.busy": "2025-11-02T09:51:38.496114Z",
     "iopub.status.idle": "2025-11-02T09:51:38.552175Z",
     "shell.execute_reply": "2025-11-02T09:51:38.551597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in synthetic mode with random tensors for debugging.\n",
      "Train samples: 3, Validation samples: 1\n"
     ]
    }
   ],
   "source": [
    "def build_file_lists(cfg: ExperimentConfig) -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:\n",
    "    if not data_available:\n",
    "        print(\"Running in synthetic mode with random tensors for debugging.\")\n",
    "        temp_dir = Path(\"./synthetic_isles22\")\n",
    "        temp_dir.mkdir(exist_ok=True)\n",
    "        np.random.seed(cfg.seed)\n",
    "        samples = []\n",
    "        for idx in range(cfg.synthetic_samples):\n",
    "            image_path = temp_dir / f\"image_{idx}.npy\"\n",
    "            label_path = temp_dir / f\"label_{idx}.npy\"\n",
    "            np.save(image_path, np.random.rand(1, *cfg.roi_size).astype(np.float32))\n",
    "            np.save(label_path, (np.random.rand(1, *cfg.roi_size) > 0.8).astype(np.uint8))\n",
    "            samples.append({\"image\": str(image_path), \"label\": str(label_path)})\n",
    "        train_samples, val_samples = train_test_split(\n",
    "            samples, test_size=cfg.validation_split, random_state=cfg.seed\n",
    "        )\n",
    "        return train_samples, val_samples\n",
    "\n",
    "    image_dir = cfg.data_root / cfg.images_subdir\n",
    "    label_dir = cfg.data_root / cfg.labels_subdir\n",
    "    image_files = sorted([p for p in image_dir.glob(\"*.nii*\")])\n",
    "    label_files = sorted([p for p in label_dir.glob(\"*.nii*\")])\n",
    "    assert len(image_files) == len(label_files), \"Mismatch between images and labels\"\n",
    "    data = [{\"image\": str(i), \"label\": str(l)} for i, l in zip(image_files, label_files)]\n",
    "\n",
    "    if cfg.val_images_subdir and cfg.val_labels_subdir:\n",
    "        val_images = sorted((cfg.data_root / cfg.val_images_subdir).glob(\"*.nii*\"))\n",
    "        val_labels = sorted((cfg.data_root / cfg.val_labels_subdir).glob(\"*.nii*\"))\n",
    "        if val_images and val_labels:\n",
    "            val_data = [\n",
    "                {\"image\": str(i), \"label\": str(l)} for i, l in zip(val_images, val_labels)\n",
    "            ]\n",
    "            return data, val_data\n",
    "\n",
    "    train_data, val_data = train_test_split(\n",
    "        data, test_size=cfg.validation_split, random_state=cfg.seed\n",
    "    )\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "train_files, val_files = build_file_lists(cfg)\n",
    "print(f\"Train samples: {len(train_files)}, Validation samples: {len(val_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828792f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.554375Z",
     "iopub.status.busy": "2025-11-02T09:51:38.554271Z",
     "iopub.status.idle": "2025-11-02T09:51:38.556954Z",
     "shell.execute_reply": "2025-11-02T09:51:38.556552Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoadVolume(MapTransform):\n",
    "    \"\"\"Load NIfTI or NumPy volumes into memory.\"\"\"\n",
    "\n",
    "    def __init__(self, keys: Sequence[str]):\n",
    "        super().__init__(keys)\n",
    "        self.nifti_loader = LoadImaged(keys=keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            path = d[key]\n",
    "            if str(path).endswith(\".npy\"):\n",
    "                d[key] = np.load(path)\n",
    "            else:\n",
    "                loaded = self.nifti_loader({key: path})\n",
    "                d[key] = loaded[key]\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577c099e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.559881Z",
     "iopub.status.busy": "2025-11-02T09:51:38.559740Z",
     "iopub.status.idle": "2025-11-02T09:51:38.569245Z",
     "shell.execute_reply": "2025-11-02T09:51:38.568670Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_transforms(cfg: ExperimentConfig, augment: bool = True):\n",
    "    load = LoadVolume(keys=[\"image\", \"label\"])\n",
    "    common = [\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"], dtype=torch.float32),\n",
    "    ]\n",
    "    if data_available:\n",
    "        intensity = ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-200,\n",
    "            a_max=200,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        )\n",
    "    else:\n",
    "        intensity = ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=1.0, b_min=0.0, b_max=1.0)\n",
    "\n",
    "    aug = []\n",
    "    if augment:\n",
    "        aug.extend(\n",
    "            [\n",
    "                RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=cfg.roi_size, random_size=False),\n",
    "                RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "                RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "                RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "                RandZoomd(keys=[\"image\", \"label\"], prob=0.2, min_zoom=0.9, max_zoom=1.1),\n",
    "                RandAffined(\n",
    "                    keys=[\"image\", \"label\"],\n",
    "                    prob=0.15,\n",
    "                    translate_range=[10, 10, 5],\n",
    "                    rotate_range=[np.pi / 18] * 3,\n",
    "                    padding_mode=\"border\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        aug.append(\n",
    "            RandSpatialCropd(\n",
    "                keys=[\"image\", \"label\"], roi_size=cfg.roi_size, random_size=False, random_center=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    post = [EnsureTyped(keys=[\"image\", \"label\"], dtype=torch.float32)]\n",
    "\n",
    "    return Compose([load, *common, intensity, *aug, *post])\n",
    "\n",
    "\n",
    "def create_datasets(cfg: ExperimentConfig):\n",
    "    train_transforms = get_transforms(cfg, augment=True)\n",
    "    val_transforms = get_transforms(cfg, augment=False)\n",
    "\n",
    "    train_ds = CacheDataset(train_files, transform=train_transforms, cache_rate=cfg.cache_rate)\n",
    "    val_ds = CacheDataset(val_files, transform=val_transforms, cache_rate=cfg.cache_rate)\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, batch_size, shuffle=True, sample_weights=None):\n",
    "    if sample_weights is not None:\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(\n",
    "            weights=torch.tensor(sample_weights, dtype=torch.double),\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True,\n",
    "        )\n",
    "        shuffle = False\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle and sampler is None,\n",
    "        sampler=sampler,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "\n",
    "train_ds, val_ds = create_datasets(cfg)\n",
    "train_loader = create_dataloader(train_ds, cfg.batch_size, shuffle=True)\n",
    "val_loader = create_dataloader(val_ds, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1068fa21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.571153Z",
     "iopub.status.busy": "2025-11-02T09:51:38.571039Z",
     "iopub.status.idle": "2025-11-02T09:51:38.574899Z",
     "shell.execute_reply": "2025-11-02T09:51:38.573613Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_unet(cfg: ExperimentConfig) -> nn.Module:\n",
    "    model = UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=cfg.in_channels,\n",
    "        out_channels=cfg.num_classes,\n",
    "        channels=(16, 32, 64, 128, 256),\n",
    "        strides=(2, 2, 2, 2),\n",
    "        num_res_units=2,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "678ab6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.576830Z",
     "iopub.status.busy": "2025-11-02T09:51:38.576690Z",
     "iopub.status.idle": "2025-11-02T09:51:38.585794Z",
     "shell.execute_reply": "2025-11-02T09:51:38.584593Z"
    }
   },
   "outputs": [],
   "source": [
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "hausdorff_metric = HausdorffDistanceMetric(include_background=False, percentile=95)\n",
    "post_trans = Compose(\n",
    "    [\n",
    "        Activationsd(keys=\"pred\", softmax=True),\n",
    "        AsDiscreted(keys=\"pred\", argmax=True, to_onehot=cfg.num_classes),\n",
    "        AsDiscreted(keys=\"label\", to_onehot=cfg.num_classes),\n",
    "    ]\n",
    ")\n",
    "loss_fn = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler=None):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_data in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        images = batch_data[\"image\"].to(device)\n",
    "        labels = batch_data[\"label\"].to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "        if scaler is not None and cfg.amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / max(1, len(loader))\n",
    "\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    dice_metric.reset()\n",
    "    hausdorff_metric.reset()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(loader, desc=\"Validate\", leave=False):\n",
    "            images = batch_data[\"image\"].to(device)\n",
    "            labels = batch_data[\"label\"].to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            data = {\"pred\": outputs, \"label\": labels}\n",
    "            data_list = [post_trans(i) for i in decollate_batch(data)]\n",
    "            for item in data_list:\n",
    "                dice_metric(item[\"pred\"], item[\"label\"])\n",
    "                hausdorff_metric(item[\"pred\"], item[\"label\"])\n",
    "    avg_dice = dice_metric.aggregate().item()\n",
    "    avg_hd = hausdorff_metric.aggregate().item()\n",
    "    return val_loss / max(1, len(loader)), avg_dice, avg_hd\n",
    "\n",
    "\n",
    "def early_stopping_training(model, train_loader, val_loader, cfg: ExperimentConfig, run_name: str):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=cfg.base_lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.max_epochs)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n",
    "    best_score = -np.inf\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "\n",
    "    for epoch in range(cfg.max_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scaler)\n",
    "        val_loss, val_dice, val_hd = validate(model, val_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "        writer.add_scalar(\"Metrics/dice\", val_dice, epoch)\n",
    "        writer.add_scalar(\"Metrics/hausdorff\", val_hd, epoch)\n",
    "\n",
    "        score = val_dice\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{cfg.max_epochs} - Train Loss: {train_loss:.4f}, Val Dice: {val_dice:.4f}, Val HD95: {val_hd:.2f}\"\n",
    "        )\n",
    "\n",
    "        if patience_counter >= cfg.patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b783cca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.588031Z",
     "iopub.status.busy": "2025-11-02T09:51:38.587892Z",
     "iopub.status.idle": "2025-11-02T09:51:38.591216Z",
     "shell.execute_reply": "2025-11-02T09:51:38.590412Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_bagging_ensemble(cfg: ExperimentConfig, train_ds, val_loader):\n",
    "    models: List[nn.Module] = []\n",
    "    scores: List[float] = []\n",
    "    for model_idx in range(cfg.n_bagging_models):\n",
    "        bootstrap_indices = np.random.choice(len(train_ds), size=len(train_ds), replace=True)\n",
    "        sample_counts = np.bincount(bootstrap_indices, minlength=len(train_ds))\n",
    "        sample_weights = sample_counts.astype(np.float32) + 1e-3\n",
    "        loader = create_dataloader(\n",
    "            train_ds, batch_size=cfg.batch_size, shuffle=False, sample_weights=sample_weights\n",
    "        )\n",
    "        model = create_unet(cfg)\n",
    "        run_name = f\"bagging/model_{model_idx}\"\n",
    "        model, score = early_stopping_training(model, loader, val_loader, cfg, run_name)\n",
    "        models.append(model)\n",
    "        scores.append(score)\n",
    "    return models, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bdd17aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.593667Z",
     "iopub.status.busy": "2025-11-02T09:51:38.593525Z",
     "iopub.status.idle": "2025-11-02T09:51:38.598052Z",
     "shell.execute_reply": "2025-11-02T09:51:38.597068Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_evaluation_dataset(files):\n",
    "    eval_transforms = Compose(\n",
    "        [\n",
    "            LoadVolume(keys=[\"image\", \"label\"]),\n",
    "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "            EnsureTyped(keys=[\"image\", \"label\"], dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "    return CacheDataset(files, transform=eval_transforms, cache_rate=0.0)\n",
    "\n",
    "\n",
    "def compute_sample_errors(model: nn.Module, files: List[Dict[str, str]]):\n",
    "    eval_ds = build_evaluation_dataset(files)\n",
    "    loader = DataLoader(eval_ds, batch_size=1, shuffle=False, num_workers=cfg.num_workers)\n",
    "    errors = []\n",
    "    metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            images = batch_data[\"image\"].to(device)\n",
    "            labels = batch_data[\"label\"].to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "                outputs = model(images)\n",
    "            data = {\"pred\": outputs, \"label\": labels}\n",
    "            data_list = [post_trans(i) for i in decollate_batch(data)]\n",
    "            sample_dice = metric(data_list[0][\"pred\"], data_list[0][\"label\"]).item()\n",
    "            errors.append(1.0 - sample_dice)\n",
    "            metric.reset()\n",
    "    return np.array(errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccfc912a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.600626Z",
     "iopub.status.busy": "2025-11-02T09:51:38.600487Z",
     "iopub.status.idle": "2025-11-02T09:51:38.604913Z",
     "shell.execute_reply": "2025-11-02T09:51:38.603942Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_boosting_ensemble(cfg: ExperimentConfig, train_ds, val_loader, train_files):\n",
    "    n_samples = len(train_ds)\n",
    "    weights = np.ones(n_samples, dtype=np.float32) / max(1, n_samples)\n",
    "    models: List[nn.Module] = []\n",
    "    alphas: List[float] = []\n",
    "    scores: List[float] = []\n",
    "\n",
    "    for round_idx in range(cfg.boosting_rounds):\n",
    "        loader = create_dataloader(\n",
    "            train_ds, batch_size=cfg.batch_size, shuffle=False, sample_weights=weights\n",
    "        )\n",
    "        model = create_unet(cfg)\n",
    "        run_name = f\"boosting/round_{round_idx}\"\n",
    "        model, score = early_stopping_training(model, loader, val_loader, cfg, run_name)\n",
    "        models.append(model)\n",
    "        scores.append(score)\n",
    "\n",
    "        errors = compute_sample_errors(model, train_files)\n",
    "        weighted_error = np.clip(np.average(errors, weights=weights), 1e-6, 1 - 1e-6)\n",
    "        alpha = 0.5 * np.log((1 - weighted_error) / weighted_error)\n",
    "        alphas.append(alpha)\n",
    "\n",
    "        misclassified = (errors > 0.5).astype(np.float32)\n",
    "        weights = weights * np.exp(alpha * (2 * misclassified - 1))\n",
    "        weights = weights / weights.sum()\n",
    "        print(\n",
    "            f\"Boosting round {round_idx+1}: weighted error={weighted_error:.4f}, alpha={alpha:.3f}, best val dice={score:.4f}\"\n",
    "        )\n",
    "\n",
    "    return models, alphas, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b14bda9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.606954Z",
     "iopub.status.busy": "2025-11-02T09:51:38.606813Z",
     "iopub.status.idle": "2025-11-02T09:51:38.612510Z",
     "shell.execute_reply": "2025-11-02T09:51:38.611096Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_meta_features(models: List[nn.Module], loader, max_samples: int = 200000):\n",
    "    feature_list = []\n",
    "    target_list = []\n",
    "    collected = 0\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            images = batch_data[\"image\"].to(device)\n",
    "            labels = batch_data[\"label\"].cpu().numpy()\n",
    "            probs = []\n",
    "            for model in models:\n",
    "                with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "                    output = torch.softmax(model(images), dim=1).cpu().numpy()\n",
    "                probs.append(output)\n",
    "            probs = np.stack(probs, axis=0)  # ensemble x batch x C x ...\n",
    "            batch_size = probs.shape[1]\n",
    "            for b in range(batch_size):\n",
    "                target = labels[b]\n",
    "                ensemble_probs = probs[:, b]\n",
    "                ensemble_probs = np.moveaxis(ensemble_probs, 0, -1)  # shape: C x ... x ensemble\n",
    "                ensemble_probs = np.reshape(ensemble_probs, (cfg.num_classes, -1, len(models)))\n",
    "                target_flat = target.reshape(-1).astype(np.int64)\n",
    "\n",
    "                idx = np.random.permutation(target_flat.size)\n",
    "                for i in idx:\n",
    "                    feature = ensemble_probs[:, i, :].reshape(-1).astype(np.float32)\n",
    "                    feature_list.append(feature)\n",
    "                    target_list.append(target_flat[i])\n",
    "                    collected += 1\n",
    "                    if collected >= max_samples:\n",
    "                        return np.array(feature_list), np.array(target_list)\n",
    "    return np.array(feature_list), np.array(target_list)\n",
    "\n",
    "\n",
    "def train_stacking_meta_learner(base_models: List[nn.Module], val_loader):\n",
    "    X, y = collect_meta_features(base_models, val_loader)\n",
    "    if X.size == 0:\n",
    "        raise RuntimeError(\"No meta-features collected; ensure validation loader has data.\")\n",
    "    meta_model = LogisticRegression(\n",
    "        max_iter=cfg.stacking_meta_epochs,\n",
    "        multi_class=\"multinomial\",\n",
    "        class_weight=\"balanced\",\n",
    "        solver=\"saga\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    meta_model.fit(X, y)\n",
    "    return meta_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fb1b37c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.614917Z",
     "iopub.status.busy": "2025-11-02T09:51:38.614778Z",
     "iopub.status.idle": "2025-11-02T09:51:38.621172Z",
     "shell.execute_reply": "2025-11-02T09:51:38.620106Z"
    }
   },
   "outputs": [],
   "source": [
    "def bagging_inference(models: List[nn.Module], images: torch.Tensor):\n",
    "    probs = []\n",
    "    for model in models:\n",
    "        with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "            output = torch.softmax(model(images), dim=1)\n",
    "        probs.append(output)\n",
    "    return torch.mean(torch.stack(probs, dim=0), dim=0)\n",
    "\n",
    "\n",
    "def boosting_inference(models: List[nn.Module], alphas: List[float], images: torch.Tensor):\n",
    "    weighted_sum = 0.0\n",
    "    alpha_sum = 0.0\n",
    "    for model, alpha in zip(models, alphas):\n",
    "        with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "            output = torch.softmax(model(images), dim=1)\n",
    "        weighted_sum = weighted_sum + alpha * output\n",
    "        alpha_sum += alpha\n",
    "    return weighted_sum / max(alpha_sum, 1e-6)\n",
    "\n",
    "\n",
    "def stacking_inference(models: List[nn.Module], meta_model: LogisticRegression, images: torch.Tensor):\n",
    "    base_probs = []\n",
    "    for model in models:\n",
    "        with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "            prob = torch.softmax(model(images), dim=1)\n",
    "        base_probs.append(prob.cpu().numpy())\n",
    "    base_probs = np.stack(base_probs, axis=0)\n",
    "    batch_preds = []\n",
    "    batch_size = images.shape[0]\n",
    "    for b in range(batch_size):\n",
    "        sample_probs = base_probs[:, b]\n",
    "        sample_probs = np.moveaxis(sample_probs, 0, -1)\n",
    "        sample_probs = np.reshape(sample_probs, (cfg.num_classes, -1, len(models)))\n",
    "        features = sample_probs.reshape(-1, len(models) * cfg.num_classes)\n",
    "        meta_pred = meta_model.predict_proba(features)\n",
    "        meta_pred = meta_pred.reshape(-1, cfg.num_classes).astype(np.float32).T\n",
    "        meta_pred = meta_pred.reshape(cfg.num_classes, *images.shape[2:])\n",
    "        batch_preds.append(torch.from_numpy(meta_pred))\n",
    "    return torch.stack(batch_preds, dim=0).to(device)\n",
    "\n",
    "\n",
    "def evaluate_ensemble(predict_fn, loader):\n",
    "    dice_metric.reset()\n",
    "    hausdorff_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            images = batch_data[\"image\"].to(device)\n",
    "            labels = batch_data[\"label\"].to(device)\n",
    "            probs = predict_fn(images)\n",
    "            data = decollate_batch({\"pred\": probs, \"label\": labels})\n",
    "            data = [post_trans(i) for i in data]\n",
    "            for item in data:\n",
    "                dice_metric(item[\"pred\"], item[\"label\"])\n",
    "                hausdorff_metric(item[\"pred\"], item[\"label\"])\n",
    "    return dice_metric.aggregate().item(), hausdorff_metric.aggregate().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "863ba280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.623959Z",
     "iopub.status.busy": "2025-11-02T09:51:38.623821Z",
     "iopub.status.idle": "2025-11-02T09:51:38.626877Z",
     "shell.execute_reply": "2025-11-02T09:51:38.625862Z"
    }
   },
   "outputs": [],
   "source": [
    "RUN_TRAINING = False  # Set to True to launch full training once data is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72393678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T09:51:38.629387Z",
     "iopub.status.busy": "2025-11-02T09:51:38.629250Z",
     "iopub.status.idle": "2025-11-02T09:51:38.633638Z",
     "shell.execute_reply": "2025-11-02T09:51:38.632366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dry-run mode. Set RUN_TRAINING=True once the ISLES'22 dataset is available.\n"
     ]
    }
   ],
   "source": [
    "if RUN_TRAINING:\n",
    "    print(\"Starting bagging ensemble training...\")\n",
    "    bagging_models, bagging_scores = train_bagging_ensemble(cfg, train_ds, val_loader)\n",
    "    bagging_dice, bagging_hd = evaluate_ensemble(\n",
    "        lambda imgs: bagging_inference(bagging_models, imgs), val_loader\n",
    "    )\n",
    "    print(f\"Bagging ensemble - Dice: {bagging_dice:.4f}, HD95: {bagging_hd:.2f}\")\n",
    "\n",
    "    print(\"\\nStarting boosting ensemble training...\")\n",
    "    boosting_models, boosting_alphas, boosting_scores = train_boosting_ensemble(\n",
    "        cfg, train_ds, val_loader, train_files\n",
    "    )\n",
    "    boosting_dice, boosting_hd = evaluate_ensemble(\n",
    "        lambda imgs: boosting_inference(boosting_models, boosting_alphas, imgs), val_loader\n",
    "    )\n",
    "    print(f\"Boosting ensemble - Dice: {boosting_dice:.4f}, HD95: {boosting_hd:.2f}\")\n",
    "\n",
    "    print(\"\\nTraining stacking ensemble using bagging base models...\")\n",
    "    meta_model = train_stacking_meta_learner(bagging_models, val_loader)\n",
    "    stacking_dice, stacking_hd = evaluate_ensemble(\n",
    "        lambda imgs: stacking_inference(bagging_models, meta_model, imgs), val_loader\n",
    "    )\n",
    "    print(f\"Stacking ensemble - Dice: {stacking_dice:.4f}, HD95: {stacking_hd:.2f}\")\n",
    "else:\n",
    "    print(\"Dry-run mode. Set RUN_TRAINING=True once the ISLES'22 dataset is available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133007f7",
   "metadata": {},
   "source": [
    "## 2. Next steps\n",
    "\n",
    "1. Download the pre-processed ISLES'22 dataset from Google Drive into the directory referenced by `cfg.data_root`.\n",
    "2. Verify GPU availability and adjust `cfg.batch_size`, `cfg.roi_size`, and `cfg.cache_rate` to fit memory constraints.\n",
    "3. Set `RUN_TRAINING = True` in the control cell and execute the notebook sequentially.\n",
    "4. Monitor the TensorBoard logs under `runs/` for loss curves and metrics.\n",
    "5. Optionally persist trained weights and ensemble metadata (alphas, logistic regression coefficients) for inference.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
